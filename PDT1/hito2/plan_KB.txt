En el hito 2, el objetivo es tener un prototipo automatizado que reciba el JSON schema de Command y genere los reportes individuales usando modelos de lenguaje. 

Para ello, dado que el objetivo final es usar SLM, se elaborará una KB con las recomendaciones asociadas por rol, gravedad y amenaza. Intentando asignarles un paso a paso de acciones para aplicarlas y un nivel de complejidad y efectividad.

Para elaborar la KB se requieren varios pasos y se usará un LLMs como Gemini o ChatGPT.
1. Ingesta de documentos
- Para empezar se deben recopilar documentos, páginas o árticulos que mencionen recomendaciones e instrucciones para mejorar la ciberhigiene y ciberseguridad de los usuarios.
- Luego se necesita un método automatizado para leer esos documentos (PDFs, Word, HTML, Markdown).
- Tras extraer el texto de los documentos, se deben extraer metadatos, entidades, relaciones, clasificación, etc. Para poder agruparlos.

2. Representación / indexado semántico
- Con los textos agrupados, se deben crear embeddings de los fragmentos de texto (chunks). Se puede usar vector store (e.g. FAISS, Milvus, Qdrant, Weaviate, Pinecone).

3. Módulo de recuperación (Retriever)
- Ya con los embeddings elaborados, se debe desarrollar un módulo para recuperar los fragmentos del texto a partir un query o prompt. Técnicas de expansión, reranking, filtrado, deduplicación, scoring híbrido, son técnicas que podrían servir para ello. Deben tenerse en cuenta los metadatos.

4. Combinador / pipeline de generación (Generator / RAG fusion)
- En este paso se requiere combinar lo recuperado + el prompt del usuario + contexto adicional, y aplicar el modelo de lenguaje para producir la respuesta.
- Manejar citaciones / atribuciones de fuentes (source attribution).
- Validaciones, mantenimiento, monitoreo, retroalimentación y actualización

OPCIONAL. Realizar una interfaz gráfica o exponer una API para las consultas y monitorear la KB fácilmente.
- Posible interfaz de exploración de la KB, inspección de fuentes, trazabilidad.


FRAMEWORKS para partir:
- LangChain: Permite leer documentos, obtener chains, integrarse con embeddings y manejar prompts. No genera la KB completa.
- LlamaIndex: Facilita la construcción de índices y la lógica de recuperación semántica para LLMs. Puede requerir ajustes de escalabilidad para cargas grandes o producción.
- Haystack (Deepset): Permite crear pipelines completas de ingestión, recuperación y generación orientadas a producción. Requiere ajustes finos para despliegues complejos.
- UltraRAG: Automatiza y modulariza el flujo completo de conocimiento. Adopción industrial limitada y algunas integraciones pueden faltar.
- RAG Foundry: Unifica creación de datos, entrenamiento, inferencia y evaluación en RAG. Orientado más a investigación; producción necesita personalización.
- SimplyRetrieve: Proporciona recuperación ligera con GUI y API, ideal para KB locales privadas. No pensada para alta escala ni despliegues distribuidos.
- KIMAs: Maneja escenarios de conversación compleja con múltiples agentes y rutas de recuperación. Puede ser excesivo para casos simples.
- EmbedChain: Facilita la creación de chatbots y la recolección de conocimiento con embeddings. Carece de algunas funcionalidades enterprise como monitoreo y despliegue escalable.
- RAGFlow: Ofrece comprensión profunda de documentos con pipelines preconfiguradas. Podría requerir adaptaciones para dominios específicos.


PIPELINE:
1. INGESTA: LlamaIndex o LangChain para loaders de documentos. 
2. ENRIQUECIMIENTO SEMÁNTICO: Usar LLM para extraer entidades (Roles, prácticas, vulnerabilidades, controles, sectores críticos), asignar taxonomías, generar tripletes tipo ACTOR-ACTION-CONTROL. Modelos de embeddings sobre frases tipo “no actualizar contraseñas” → “uso de credenciales compartidas”.
3. Indexado: Vector Store con FAISS, Chroma, Qdrant o Milvus. Crear ontología con persona, rol, comportamiento, riesgo, control, vulnerabilidad, intervención. Asignar un risk score a cada fragmento o práctica según gravedad, frecuencia, impacto.
4. Consulta con RAG: Haystack o LlamaIndex.
5. Validación manual.


ESTRUCTURA CÓDIGO:
hfshield-rag/
│
├── docs/
│   ├── normativas/
│   ├── politicas/
│   ├── html/
│   ├── md/
│   ├── hfshield_data/
│
├── scripts/
│   └── ingest.py
│
└── qdrant_data/


from typing import List
from langchain.schema import Document

def ingest_all_documents() -> List[Document]:
    loaders = {
        "pdf": DirectoryLoader("docs/normativas", glob="**/*.pdf", loader_cls=PyPDFLoader),
        "word": DirectoryLoader("docs/politicas", glob="**/*.docx", loader_cls=UnstructuredWordDocumentLoader),
        "html": DirectoryLoader("docs/html", glob="**/*.html", loader_cls=UnstructuredHTMLLoader),
        "md": DirectoryLoader("docs/md", glob="**/*.md", loader_cls=TextLoader)
    }

    docs = []
    for name, loader in loaders.items():
        print(f"Cargando {name}...")
        docs.extend(loader.load())

    # Enriquecer metadatos
    for d in docs:
        d.metadata["categoria"] = name

    # Split
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    return splitter.split_documents(docs)


{
  "categoria": "conductual",
  "roles": ["empleado", "supervisor"],
  "riesgos": ["uso indebido de contraseñas", "negligencia"],
  "controles": ["capacitación continua", "política de contraseñas seguras"],
  "fuente": "enisa_2024_guidelines.pdf"
}

